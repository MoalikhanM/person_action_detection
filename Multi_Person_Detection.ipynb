{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbfef86",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66274c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2dea1",
   "metadata": {},
   "source": [
    "## 2. GPU Check & Prevent Memory Throw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88f7421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpu_check = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpu_check:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(gpu_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2438ad",
   "metadata": {},
   "source": [
    "# 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd452789",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load('https://tfhub.dev/google/movenet/multipose/lightning/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65367404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the model\n",
    "exr_movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76e1aa",
   "metadata": {},
   "source": [
    "# 4. Make Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf763ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to loop through each person detected and render\n",
    "def loop_through_people(frame, keypoints_with_scores, edges, confidence_threshold):\n",
    "    for person in keypoints_with_scores:\n",
    "        draw_connections(frame, person, edges, confidence_threshold)\n",
    "        draw_keypoints(frame, person, confidence_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bf640a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Video Capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Resize image\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 256, 256) # recommended minimum shape size is 256, shape size is dependent on frame\n",
    "    \n",
    "    # Convert image to integer\n",
    "    convert_image = tf.cast(img, dtype=tf.int32)\n",
    "    \n",
    "    # Detection\n",
    "    captured_res = exr_movenet(convert_image)\n",
    "    keypoints_with_scores = captured_res['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    #print(captured_res)\n",
    "    \n",
    "    # Render keypoints \n",
    "    loop_through_people(frame, keypoints_with_scores, EDGES, 0.1) # edges is defined of confidence threshold\n",
    "    \n",
    "    cv2.imshow('Movenet Multipose', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "990d4fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[217, 201, 191],\n",
       "        [217, 201, 191],\n",
       "        [217, 201, 191],\n",
       "        ...,\n",
       "        [116, 112, 110],\n",
       "        [116, 112, 110],\n",
       "        [116, 112, 110]],\n",
       "\n",
       "       [[217, 201, 191],\n",
       "        [217, 201, 191],\n",
       "        [217, 201, 191],\n",
       "        ...,\n",
       "        [117, 112, 110],\n",
       "        [118, 112, 110],\n",
       "        [118, 112, 110]],\n",
       "\n",
       "       [[219, 201, 191],\n",
       "        [218, 201, 191],\n",
       "        [217, 201, 191],\n",
       "        ...,\n",
       "        [116, 112, 110],\n",
       "        [116, 112, 110],\n",
       "        [116, 112, 110]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  9,   9,   9],\n",
       "        [  9,   9,   9],\n",
       "        [  9,   9,   9],\n",
       "        ...,\n",
       "        [ 57,  44,  35],\n",
       "        [ 77,  65,  57],\n",
       "        [ 98,  86,  78]],\n",
       "\n",
       "       [[  9,   9,   9],\n",
       "        [  9,   9,   9],\n",
       "        [  9,   9,   9],\n",
       "        ...,\n",
       "        [ 94,  81,  72],\n",
       "        [ 56,  44,  36],\n",
       "        [ 34,  22,  14]],\n",
       "\n",
       "       [[  9,   9,   9],\n",
       "        [  9,   9,   9],\n",
       "        [  9,   9,   9],\n",
       "        ...,\n",
       "        [127, 113, 104],\n",
       "        [135, 121, 113],\n",
       "        [117, 103,  95]]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bcfbbc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection via Video\n",
    "cap = cv2.VideoCapture('#filename.mp4') # the longest side of the video should be a minimum of 256 pixels and both values need to be a multipile of 32 \n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Resize image\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 384, 640) # recommended minimum shape size is 256, shape size is dependent on frame\n",
    "    \n",
    "    # Convert image to integer\n",
    "    convert_image = tf.cast(img, dtype=tf.int32)\n",
    "    \n",
    "    # Detection\n",
    "    captured_res = exr_movenet(convert_image)\n",
    "    keypoints_with_scores = captured_res['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    #print(captured_res)\n",
    "    \n",
    "    # Render keypoints \n",
    "    loop_through_people(frame, keypoints_with_scores, EDGES, 0.1) # edges is defined of confidence threshold\n",
    "    \n",
    "    cv2.imshow('Movenet Multipose', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e73442aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check a frame size\n",
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24a7d492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get similar aspect ratio to existing captured image\n",
    "1080 / 1920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd0f3f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "360 / 0.5625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19110c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.25"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values need to be a multipile of 32 \n",
    "360 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad54ac4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approximate value\n",
    "32 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29477c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.25835884, 0.5408215 , 0.05128162],\n",
       "        [0.24413782, 0.53405464, 0.05231812],\n",
       "        [0.24122077, 0.53682107, 0.02042377],\n",
       "        [0.22610329, 0.5441764 , 0.0272614 ],\n",
       "        [0.21237728, 0.56046075, 0.00351454],\n",
       "        [0.28716594, 0.57216674, 0.586503  ],\n",
       "        [0.25457135, 0.64240664, 0.2207593 ],\n",
       "        [0.4229686 , 0.58244807, 0.42630887],\n",
       "        [0.3455741 , 0.7109286 , 0.3591295 ],\n",
       "        [0.48520324, 0.5320727 , 0.30815354],\n",
       "        [0.43212816, 0.7534434 , 0.02635468],\n",
       "        [0.4918021 , 0.6258785 , 0.92411363],\n",
       "        [0.48787254, 0.670015  , 0.87747365],\n",
       "        [0.6930932 , 0.5859362 , 0.48881927],\n",
       "        [0.6957552 , 0.6709063 , 0.75257313],\n",
       "        [0.8826117 , 0.5557196 , 0.1138115 ],\n",
       "        [0.8701302 , 0.691549  , 0.53081506]],\n",
       "\n",
       "       [[0.44438303, 0.16775623, 0.41339478],\n",
       "        [0.44094476, 0.17436382, 0.4793458 ],\n",
       "        [0.43957376, 0.16225688, 0.38548675],\n",
       "        [0.44962955, 0.1819278 , 0.4439863 ],\n",
       "        [0.44872457, 0.15537277, 0.36913902],\n",
       "        [0.48537225, 0.19136499, 0.45437282],\n",
       "        [0.4763689 , 0.14131542, 0.3276515 ],\n",
       "        [0.52140623, 0.20283543, 0.1932343 ],\n",
       "        [0.48738462, 0.13768306, 0.17258085],\n",
       "        [0.5384639 , 0.184745  , 0.17300552],\n",
       "        [0.5554217 , 0.1396377 , 0.17709668],\n",
       "        [0.5702677 , 0.17972147, 0.34235042],\n",
       "        [0.557542  , 0.1506611 , 0.31965   ],\n",
       "        [0.6068021 , 0.19376722, 0.09672739],\n",
       "        [0.5938376 , 0.13978627, 0.10821629],\n",
       "        [0.63334167, 0.18732405, 0.08008341],\n",
       "        [0.6395469 , 0.15070133, 0.08253682]],\n",
       "\n",
       "       [[0.29737705, 0.11947455, 0.604576  ],\n",
       "        [0.29110837, 0.12285291, 0.51367384],\n",
       "        [0.29116508, 0.11624523, 0.45528308],\n",
       "        [0.29602772, 0.12468628, 0.5915977 ],\n",
       "        [0.29607984, 0.11107384, 0.6257863 ],\n",
       "        [0.32737547, 0.13430145, 0.63193285],\n",
       "        [0.33186534, 0.10067284, 0.53932565],\n",
       "        [0.36557394, 0.1418073 , 0.28887168],\n",
       "        [0.38327926, 0.09154805, 0.38423243],\n",
       "        [0.3600025 , 0.13415119, 0.28450456],\n",
       "        [0.36844015, 0.11971158, 0.28856912],\n",
       "        [0.40237376, 0.12804757, 0.50030905],\n",
       "        [0.4138931 , 0.10887223, 0.43327627],\n",
       "        [0.4666029 , 0.13999057, 0.2294097 ],\n",
       "        [0.4513368 , 0.10210949, 0.22865675],\n",
       "        [0.48314923, 0.13841312, 0.24766007],\n",
       "        [0.50117606, 0.09604193, 0.2025269 ]],\n",
       "\n",
       "       [[0.2896884 , 0.17163184, 0.6328783 ],\n",
       "        [0.2825868 , 0.17663072, 0.47323143],\n",
       "        [0.28250736, 0.16965789, 0.42943853],\n",
       "        [0.28634107, 0.18659511, 0.57589954],\n",
       "        [0.28552425, 0.17096958, 0.71335196],\n",
       "        [0.31771493, 0.20229976, 0.87053996],\n",
       "        [0.32925463, 0.15833141, 0.57542473],\n",
       "        [0.37071556, 0.20717596, 0.4261029 ],\n",
       "        [0.36549783, 0.14557208, 0.24741574],\n",
       "        [0.3784287 , 0.19634472, 0.39829412],\n",
       "        [0.37929377, 0.18313648, 0.3092564 ],\n",
       "        [0.4081458 , 0.18793301, 0.3995362 ],\n",
       "        [0.4071922 , 0.15978473, 0.2649012 ],\n",
       "        [0.40940914, 0.21389614, 0.07192404],\n",
       "        [0.4682648 , 0.13952716, 0.27297223],\n",
       "        [0.47818816, 0.1850734 , 0.08413281],\n",
       "        [0.42888296, 0.14728384, 0.1694699 ]],\n",
       "\n",
       "       [[0.45168018, 0.23375514, 0.6114375 ],\n",
       "        [0.44510633, 0.23869233, 0.5155738 ],\n",
       "        [0.4446277 , 0.23020534, 0.5256365 ],\n",
       "        [0.4504257 , 0.24339548, 0.5034136 ],\n",
       "        [0.4517878 , 0.22674592, 0.5059942 ],\n",
       "        [0.48809886, 0.25330272, 0.49879622],\n",
       "        [0.4898156 , 0.2169607 , 0.67713606],\n",
       "        [0.5238974 , 0.26175866, 0.22222349],\n",
       "        [0.5167415 , 0.21194182, 0.4282845 ],\n",
       "        [0.5571698 , 0.26605016, 0.13813627],\n",
       "        [0.53338087, 0.22613256, 0.22170313],\n",
       "        [0.5470507 , 0.2504681 , 0.38341734],\n",
       "        [0.5686033 , 0.228806  , 0.32399642],\n",
       "        [0.59739125, 0.2767283 , 0.16476847],\n",
       "        [0.59842074, 0.22755586, 0.27353352],\n",
       "        [0.56195784, 0.24771777, 0.17123987],\n",
       "        [0.56580734, 0.23353319, 0.21908459]],\n",
       "\n",
       "       [[0.45016158, 0.31616604, 0.65539575],\n",
       "        [0.44289345, 0.32029727, 0.7241685 ],\n",
       "        [0.44243783, 0.3130153 , 0.608087  ],\n",
       "        [0.44643974, 0.32515103, 0.48624942],\n",
       "        [0.44638747, 0.30691326, 0.3918216 ],\n",
       "        [0.48778304, 0.338511  , 0.7429707 ],\n",
       "        [0.49250174, 0.28460145, 0.56454396],\n",
       "        [0.5461229 , 0.34107882, 0.48005286],\n",
       "        [0.5493959 , 0.2765309 , 0.5069862 ],\n",
       "        [0.5591077 , 0.3341734 , 0.47660765],\n",
       "        [0.5605901 , 0.29602417, 0.45628178],\n",
       "        [0.5809351 , 0.32626677, 0.6209064 ],\n",
       "        [0.58431786, 0.29423544, 0.39428258],\n",
       "        [0.6102899 , 0.33754522, 0.12350529],\n",
       "        [0.62912285, 0.293246  , 0.21549934],\n",
       "        [0.59515554, 0.32671428, 0.14389147],\n",
       "        [0.6554393 , 0.2933022 , 0.20454279]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check array values of captured image \n",
    "# in the format y_0, x_0, s_0, y_1, x_1, s_1 \n",
    "# the order of 17 keypoint: [nose, left_eye, right_eye, left_ear, right_ear, \n",
    "# left_shoulder, right_shoulder, left_elbow, right_elbow, left_wrist, right_wrist,\n",
    "# left_hip , right_hip, left_knee, right_knee, left_ankle & right_ankle]\n",
    " \n",
    "keypoints_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d735522",
   "metadata": {},
   "source": [
    "# 3. Draw Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40a450f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw keypoints for each person and action\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 6, (0,255,0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e97f93",
   "metadata": {},
   "source": [
    "# 4. Draw Edges Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4481ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges of joints, eg. 0 is nose and 1 is left eye, so m is a connection of\n",
    "# nose and left eye\n",
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6ec7bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render results\n",
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
